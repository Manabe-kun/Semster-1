{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu117\n",
      "Requirement already satisfied: torch in c:\\users\\kalyan\\win-virtualenvs\\bigdata_ds\\lib\\site-packages (2.0.0+cu117)\n",
      "Requirement already satisfied: torchvision in c:\\users\\kalyan\\win-virtualenvs\\bigdata_ds\\lib\\site-packages (0.15.1+cu117)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\kalyan\\win-virtualenvs\\bigdata_ds\\lib\\site-packages (2.0.1+cu117)\n",
      "Requirement already satisfied: filelock in c:\\users\\kalyan\\win-virtualenvs\\bigdata_ds\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\kalyan\\win-virtualenvs\\bigdata_ds\\lib\\site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\kalyan\\win-virtualenvs\\bigdata_ds\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\kalyan\\win-virtualenvs\\bigdata_ds\\lib\\site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kalyan\\win-virtualenvs\\bigdata_ds\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\kalyan\\win-virtualenvs\\bigdata_ds\\lib\\site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: requests in c:\\users\\kalyan\\win-virtualenvs\\bigdata_ds\\lib\\site-packages (from torchvision) (2.28.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\kalyan\\win-virtualenvs\\bigdata_ds\\lib\\site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kalyan\\win-virtualenvs\\bigdata_ds\\lib\\site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kalyan\\win-virtualenvs\\bigdata_ds\\lib\\site-packages (from requests->torchvision) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kalyan\\win-virtualenvs\\bigdata_ds\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kalyan\\win-virtualenvs\\bigdata_ds\\lib\\site-packages (from requests->torchvision) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kalyan\\win-virtualenvs\\bigdata_ds\\lib\\site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\kalyan\\win-virtualenvs\\bigdata_ds\\lib\\site-packages (from sympy->torch) (1.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1797\n",
      "1797\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c888b89b50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Required Imports\n",
    "from sklearn import *\n",
    "from sklearn.model_selection import *\n",
    "import torch\n",
    "import torch.nn as Neurals\n",
    "import torch.nn.functional as NeuralsFunctional\n",
    "import torch.optim as optimizer\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "'''\n",
    "import torch\n",
    "import torch.Neurals as Neurals\n",
    "import torch.Neurals.functional as NeuralsFunctional\n",
    "import torch.optim as optimizer\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "Numbers = datasets.load_digits()\n",
    "plt.imshow(Numbers.images[-1], cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit 0: 178\n",
      "Digit 1: 182\n",
      "Digit 2: 177\n",
      "Digit 3: 183\n",
      "Digit 4: 181\n",
      "Digit 5: 182\n",
      "Digit 6: 181\n",
      "Digit 7: 179\n",
      "Digit 8: 174\n",
      "Digit 9: 180\n"
     ]
    }
   ],
   "source": [
    "NumberCount = {}\n",
    "for Number in Numbers.target:\n",
    "    if Number in NumberCount:\n",
    "        NumberCount[Number] += 1\n",
    "    else:\n",
    "        NumberCount[Number] = 1\n",
    "\n",
    "keys = list(NumberCount.keys())\n",
    "i = 0\n",
    "\n",
    "while i < len(keys):\n",
    "    digit = keys[i]\n",
    "    count = NumberCount[digit]\n",
    "    print(\"Digit\", digit, \":\", count)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Numbers.data, Numbers.target, test_size=0.3, random_state=22)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetSklearnDigits(\n",
      "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class NeuralNetworkNumbers(Neurals.Module):\n",
    "    def __init__ (self):\n",
    "        super().__init__()\n",
    "        self.fc1 = Neurals.Linear(64, 32)\n",
    "        self.fc2 = Neurals.Linear(32,10)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.view(-1, 64)\n",
    "        X = torch.relu(self.fc1(X))\n",
    "        X = self.fc2(X)\n",
    "        return X\n",
    "\n",
    "digits_model = NeuralNetworkNumbers()\n",
    "print (digits_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = optimizer.Adam(digits_model.parameters(), lr=0.02)\n",
    "\n",
    "for epoch in range(180):\n",
    "    optimizer.zero_grad()\n",
    "    output = digits_model(X_train)\n",
    "    loss = Neurals.CrossEntropyLoss()(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred = digits_model(X_test)\n",
    "accuracy = accuracy_score(y_test, torch.argmax(y_pred, axis=1))\n",
    "print(\"Test accuracy =\", accuracy*100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new class by inheriting the Dataset class.\n",
    "class CustomImgDataset(Dataset):\n",
    "    # constructor\n",
    "    def __init__(self, path, transform=None):\n",
    "        # directory where images were present\n",
    "        self.path = path\n",
    "        # get all the files in the path dir\n",
    "        self.image_paths = os.listdir(path)\n",
    "        # remove the uNeuralsecessary DS_Store file\n",
    "        self.image_paths.remove(\".DS_Store\")\n",
    "        self.transform = transform\n",
    "    \n",
    "    # modifying the inherited methods\n",
    "    def __getitem__(self, index):\n",
    "        # open the image using PIL\n",
    "        image = Image.open(os.path.join(self.path, self.image_paths[index]))\n",
    "        # use a function to set the target class\n",
    "        target = self.set_target_class(self.image_paths[index])\n",
    "        # transform the image\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, target\n",
    "    \n",
    "    # total no. of samples\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    # NeuralsFunctionalunction to set target class as Burn(1) or Not Burn(0)\n",
    "    # based on the filename.\n",
    "    # filename = img1-62.jpg - Burn(1) else Not Burn(0)\n",
    "    def set_target_class(self, filename):\n",
    "        # Prepare a list of image names - img1-62.jpg\n",
    "        burnList = [ f'img{idx}.jpg' for idx in range(1, 62+1)]\n",
    "        # Check if the given filename is present to set Burn\n",
    "        if filename in burnList:\n",
    "            return 1\n",
    "        # Else Not Burn\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "# Image transformations\n",
    "# resize and normalize the image\n",
    "transform = transforms.Compose([\n",
    "    # resize\n",
    "    transforms.Resize((200,200)),\n",
    "    # convert to tensor\n",
    "    transforms.ToTensor(),\n",
    "    # normalize\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# DIR for the burnimages dataset\n",
    "path = \"BurnImages\"\n",
    "\n",
    "dataset = CustomImgDataset(path, transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Set the path to the directory containing the images\n",
    "directory = 'path/to/images'\n",
    "\n",
    "# Create an empty list to store the image filenames\n",
    "image_filenames = []\n",
    "\n",
    "# Loop through all the image filenames\n",
    "for i in range(1, 63):\n",
    "    # Construct the filename for the current image\n",
    "    filename = os.path.join(directory, 'burn', f'img{i}.jpg')\n",
    "    \n",
    "    # Add the filename to the list\n",
    "    image_filenames.append(filename)\n",
    "\n",
    "# Loop through the list of image filenames\n",
    "for filename in image_filenames:\n",
    "    # Open the image file\n",
    "    image = Image.open(filename)\n",
    "    \n",
    "    # Display the image\n",
    "    image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set train and test sizes\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "# Split dataset into train and test sets\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size],\n",
    "                                           generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# Define the data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNeurals - Convolution neural network\n",
    "class BurnCNeurals(Neurals.Module):\n",
    "    def __init__(self):\n",
    "        super(BurnCNeurals, self).__init__()\n",
    "        self.conv1 = Neurals.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = Neurals.BatchNorm2d(32)\n",
    "        self.conv2 = Neurals.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = Neurals.BatchNorm2d(64)\n",
    "        self.conv3 = Neurals.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = Neurals.BatchNorm2d(128)\n",
    "        self.fc1 = Neurals.Linear(128*50*50, 256)\n",
    "        self.fc2 = Neurals.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = NeuralsFunctional.relu(self.bn1(self.conv1(x)))\n",
    "        x = NeuralsFunctional.max_pool2d(NeuralsFunctional.relu(self.bn2(self.conv2(x))), 2)\n",
    "        x = NeuralsFunctional.max_pool2d(NeuralsFunctional.relu(self.bn3(self.conv3(x))), 2)\n",
    "        x = x.view(-1, 128*50*50)\n",
    "        x = NeuralsFunctional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    1,     3] loss: 14.605\n",
      "[    2,     3] loss: 0.540\n",
      "[    3,     3] loss: 0.971\n",
      "[    4,     3] loss: 0.255\n",
      "[    5,     3] loss: 0.183\n"
     ]
    }
   ],
   "source": [
    "# instantiate the convolution neural network\n",
    "cNeurals = BurnCNeurals()\n",
    "\n",
    "# setup loss function and optimizer\n",
    "criterion = Neurals.BCEWithLogitsLoss()\n",
    "optimizer = optimizer.Adam(cNeurals.parameters())\n",
    "\n",
    "# train the neural network\n",
    "num_epochs = 5\n",
    "# no. of iterations/epochs to loop\n",
    "for epoch in range(num_epochs):\n",
    "    ruNeuralsing_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        # zero the param gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward -> backward -> optimize\n",
    "        outputs = cNeurals(inputs)\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print some loss statistics\n",
    "        ruNeuralsing_loss += loss.item()\n",
    "        if i % 4 == 2:\n",
    "            print(f'[{epoch+1:5d}, {i+1:5d}] loss: {ruNeuralsing_loss/10:.3f}')\n",
    "            ruNeuralsing_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 18 test images: 88.89%.\n"
     ]
    }
   ],
   "source": [
    "# test the neural network\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = cNeurals(images)\n",
    "        predicted = torch.round(torch.sigmoid(outputs))\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the {len(test_dataset)} test images: {100*correct/total:.2f}%.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
